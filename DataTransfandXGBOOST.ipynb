{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6aa6f7-7b0b-498f-868a-5fe6401ec22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/epx8hh/.conda/envs/dsvenv/lib/python3.12/site-packages (3.1.1)\n",
      "Requirement already satisfied: numpy in /home/epx8hh/.conda/envs/dsvenv/lib/python3.12/site-packages (from xgboost) (2.2.6)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/epx8hh/.conda/envs/dsvenv/lib/python3.12/site-packages (from xgboost) (2.26.2)\n",
      "Requirement already satisfied: scipy in /home/epx8hh/.conda/envs/dsvenv/lib/python3.12/site-packages (from xgboost) (1.15.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, log_loss, accuracy_score\n",
    "import xgboost as xgb\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "defensive_df      = pd.read_csv(os.path.join(\"data/defensive_2019_2023.csv\"))\n",
    "receiving_df      = pd.read_csv(os.path.join(\"data/receiving_2019_2023.csv\"))\n",
    "rushing_df        = pd.read_csv(os.path.join(\"data/rushing_2019_2023.csv\"))\n",
    "passing_df        = pd.read_csv(os.path.join(\"data/passing_2019_2023.csv\"))\n",
    "fumbles_df        = pd.read_csv(os.path.join(\"data/fumbles_2019_2023.csv\"))\n",
    "interceptions_df  = pd.read_csv(os.path.join(\"data/interceptions_2019_2023.csv\"))\n",
    "kickreturns_df    = pd.read_csv(os.path.join(\"data/kickreturns_2019_2023.csv\"))\n",
    "puntreturns_df    = pd.read_csv(os.path.join(\"data/puntreturn_2019_2023.csv\"))\n",
    "kicking_df        = pd.read_csv(os.path.join(\"data/kicking_2019_2023.csv\"))\n",
    "punting_df        = pd.read_csv(os.path.join(\"data/punting_2019_2023.csv\"))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ECE CALCULATION (from Walsh & Joshi, 2024)\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_ece(y_true, y_pred_proba, n_bins=10):\n",
    "    \"\"\"\n",
    "    Calculate Expected Calibration Error\n",
    "    \n",
    "    Args:\n",
    "        y_true: True binary labels (0 or 1)\n",
    "        y_pred_proba: Predicted probabilities [0, 1]\n",
    "        n_bins: Number of bins for calibration (default 10)\n",
    "    \n",
    "    Returns:\n",
    "        ece: Expected Calibration Error\n",
    "        bin_data: DataFrame with per-bin statistics\n",
    "    \"\"\"\n",
    "    # Create bins\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(y_pred_proba, bins) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n",
    "    \n",
    "    ece = 0.0\n",
    "    bin_data = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        bin_mask = bin_indices == i\n",
    "        \n",
    "        if bin_mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        bin_size = bin_mask.sum()\n",
    "        bin_confidence = y_pred_proba[bin_mask].mean()\n",
    "        bin_accuracy = y_true[bin_mask].mean()\n",
    "        \n",
    "        # ECE contribution\n",
    "        ece += (bin_size / len(y_true)) * abs(bin_confidence - bin_accuracy)\n",
    "        \n",
    "        bin_data.append({\n",
    "            'bin': i,\n",
    "            'bin_lower': bins[i],\n",
    "            'bin_upper': bins[i + 1],\n",
    "            'count': int(bin_size),\n",
    "            'avg_confidence': bin_confidence,\n",
    "            'avg_accuracy': bin_accuracy,\n",
    "            'calibration_error': abs(bin_confidence - bin_accuracy)\n",
    "        })\n",
    "    \n",
    "    return ece, pd.DataFrame(bin_data)\n",
    "\n",
    "\n",
    "def plot_calibration_curve(y_true, y_pred_proba, n_bins=10, title=\"Calibration Curve\"):\n",
    "    \"\"\"Plot calibration curve\"\"\"\n",
    "    ece, bin_data = calculate_ece(y_true, y_pred_proba, n_bins)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Plot perfect calibration\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    # Plot actual calibration\n",
    "    if not bin_data.empty:\n",
    "        ax.plot(bin_data['avg_confidence'], bin_data['avg_accuracy'], \n",
    "                'o-', label=f'Model (ECE={ece:.4f})')\n",
    "    \n",
    "    ax.set_xlabel('Predicted Probability')\n",
    "    ax.set_ylabel('Actual Frequency')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    return fig, ece\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_player_prop_data(df, stat_col='YDS', prop_threshold=50, additional_stat_cols=None):\n",
    "    \"\"\"Prepare data for binary prop prediction\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(['athlete_id', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    if additional_stat_cols is None:\n",
    "        exclude_cols = ['game_id', 'date', 'season', 'team', 'home_away', \n",
    "                       'opposing_team', 'athlete_id', 'display_name', 'position', 'position_abbr']\n",
    "        additional_stat_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    for col in additional_stat_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    df = df.dropna(subset=[stat_col])\n",
    "    df['prop_outcome'] = (df[stat_col] > prop_threshold).astype(int)\n",
    "    \n",
    "    # Encode categorical\n",
    "    le_team = LabelEncoder()\n",
    "    le_opp = LabelEncoder()\n",
    "    le_position = LabelEncoder()\n",
    "    \n",
    "    df['team_encoded'] = le_team.fit_transform(df['team'].fillna('UNK'))\n",
    "    df['opposing_team_encoded'] = le_opp.fit_transform(df['opposing_team'].fillna('UNK'))\n",
    "    df['position_encoded'] = le_position.fit_transform(df['position'].fillna('UNK'))\n",
    "    df['home_away_encoded'] = (df['home_away'] == 'home').astype(int)\n",
    "    \n",
    "    feature_cols = ['team_encoded', 'opposing_team_encoded', 'position_encoded', \n",
    "                    'home_away_encoded', 'season']\n",
    "    \n",
    "    # Rolling features\n",
    "    for window in [3, 5, 8]:\n",
    "        col_name = f'{stat_col.lower()}_rolling_{window}'\n",
    "        df[col_name] = df.groupby('athlete_id')[stat_col].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        feature_cols.append(col_name)\n",
    "    \n",
    "    for add_stat in additional_stat_cols:\n",
    "        if add_stat != stat_col and add_stat in df.columns:\n",
    "            for window in [3, 8]:\n",
    "                col_name = f'{add_stat.lower()}_rolling_{window}'\n",
    "                df[col_name] = df.groupby('athlete_id')[add_stat].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).mean().shift(1)\n",
    "                )\n",
    "                feature_cols.append(col_name)\n",
    "    \n",
    "    df['prop_hit_rate_8'] = df.groupby('athlete_id')['prop_outcome'].transform(\n",
    "        lambda x: x.rolling(8, min_periods=1).mean().shift(1)\n",
    "    )\n",
    "    feature_cols.append('prop_hit_rate_8')\n",
    "    \n",
    "    df[f'{stat_col.lower()}_season_avg'] = df.groupby(['athlete_id', 'season'])[stat_col].transform(\n",
    "        lambda x: x.expanding().mean().shift(1)\n",
    "    )\n",
    "    feature_cols.append(f'{stat_col.lower()}_season_avg')\n",
    "    \n",
    "    df['prop_season_hit_rate'] = df.groupby(['athlete_id', 'season'])['prop_outcome'].transform(\n",
    "        lambda x: x.expanding().mean().shift(1)\n",
    "    )\n",
    "    feature_cols.append('prop_season_hit_rate')\n",
    "    \n",
    "    df[f'{stat_col.lower()}_std_8'] = df.groupby('athlete_id')[stat_col].transform(\n",
    "        lambda x: x.rolling(8, min_periods=2).std().shift(1)\n",
    "    )\n",
    "    feature_cols.append(f'{stat_col.lower()}_std_8')\n",
    "    \n",
    "    df['trend'] = df[f'{stat_col.lower()}_rolling_3'] - df[f'{stat_col.lower()}_season_avg']\n",
    "    feature_cols.append('trend')\n",
    "    \n",
    "    df['game_num'] = df.groupby(['athlete_id', 'season']).cumcount() + 1\n",
    "    feature_cols.append('game_num')\n",
    "    \n",
    "    df['career_games'] = df.groupby('athlete_id').cumcount()\n",
    "    feature_cols.append('career_games')\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    feature_cols = list(dict.fromkeys(feature_cols))\n",
    "    \n",
    "    return df, feature_cols, le_team, le_opp, le_position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de4267b2-fab7-4003-a6e4-f681c2c02faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.48413\ttrain-auc:0.81667\ttest-logloss:0.48949\ttest-auc:0.79884\n",
      "[50]\ttrain-logloss:0.34614\ttrain-auc:0.87260\ttest-logloss:0.41529\ttest-auc:0.79983\n",
      "[55]\ttrain-logloss:0.34195\ttrain-auc:0.87670\ttest-logloss:0.41599\ttest-auc:0.79948\n",
      "\n",
      "==================================================\n",
      "=== TRAINING METRICS ===\n",
      "==================================================\n",
      "Log Loss:  0.3419\n",
      "ROC-AUC:   0.8767\n",
      "Accuracy:  0.8506\n",
      "\n",
      "==================================================\n",
      "=== TEST METRICS ===\n",
      "==================================================\n",
      "Log Loss:  0.4160\n",
      "ROC-AUC:   0.7995\n",
      "Accuracy:  0.8135\n",
      "\n",
      "‚úì Task validated: AUC > 0.60\n"
     ]
    }
   ],
   "source": [
    "# === XGBoost Binary Classification ===\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Binary classification\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': ['logloss', 'auc'],  # Log loss + AUC\n",
    "    'seed': 42,\n",
    "    'scale_pos_weight': 1  # Adjust if imbalanced\n",
    "}\n",
    "\n",
    "# Train with early stopping\n",
    "evals = [(dtrain, 'train'), (dtest, 'test')]\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# Predictions (probabilities)\n",
    "y_pred_train_proba = model.predict(dtrain)\n",
    "y_pred_test_proba = model.predict(dtest)\n",
    "\n",
    "# Binary predictions (threshold 0.5)\n",
    "y_pred_train = (y_pred_train_proba > 0.5).astype(int)\n",
    "y_pred_test = (y_pred_test_proba > 0.5).astype(int)\n",
    "\n",
    "# === Evaluation Metrics (as per image) ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== TRAINING METRICS ===\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Log Loss:  {log_loss(y_train, y_pred_train_proba):.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_train, y_pred_train_proba):.4f}\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== TEST METRICS ===\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Log Loss:  {log_loss(y_test, y_pred_test_proba):.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_test_proba):.4f}\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "# Target: AUC > 0.60 to validate task difficulty\n",
    "if roc_auc_score(y_test, y_pred_test_proba) > 0.60:\n",
    "    print(\"\\nTask validated: AUC > 0.60\")\n",
    "else:\n",
    "    print(\"\\nX Warning: AUC < 0.60, task may be too difficult or need more features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59100e77-c239-4e5b-95fa-88f940ed0d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SECTION 1: DATA LOADING\n",
      "======================================================================\n",
      "‚úÖ Loaded 10 datasets\n",
      "   Receiving: 22412 rows, 15 cols\n",
      "   Rushing:   11534 rows, 14 cols\n",
      "   Passing:   3450 rows, 17 cols\n",
      "   Defensive: 54411 rows, 16 cols\n",
      "\n",
      "Receiving columns: ['game_id', 'date', 'season', 'team', 'home_away', 'opposing_team', 'athlete_id', 'display_name', 'position', 'REC']\n",
      "\n",
      "Sample receiving data:\n",
      "     game_id        date  season                team home_away  \\\n",
      "0  401127972  2019-09-08    2019  Indianapolis Colts      away   \n",
      "1  401127972  2019-09-08    2019  Indianapolis Colts      away   \n",
      "2  401127972  2019-09-08    2019  Indianapolis Colts      away   \n",
      "\n",
      "          opposing_team  athlete_id    display_name position  REC  YDS   AVG  \\\n",
      "0  Los Angeles Chargers       14924     T.Y. Hilton       WR    8   87  10.9   \n",
      "1  Los Angeles Chargers     3728254       Deon Cain       WR    2   35  17.5   \n",
      "2  Los Angeles Chargers     2977609  Devin Funchess       TE    3   32  10.7   \n",
      "\n",
      "   TD  LONG  TGTS  \n",
      "0   2    19     9  \n",
      "1   0    25     2  \n",
      "2   0    16     5  \n",
      "\n",
      "======================================================================\n",
      "SECTION 2: ECE CALCULATION FUNCTIONS\n",
      "======================================================================\n",
      "Testing ECE calculation...\n",
      "‚úÖ ECE function works! Test ECE: 0.1800\n",
      "   Test bins:\n",
      "   bin  bin_lower  bin_upper  count  avg_confidence  avg_accuracy  \\\n",
      "0    0        0.0        0.2      2           0.125           0.0   \n",
      "1    1        0.2        0.4      2           0.250           0.0   \n",
      "2    3        0.6        0.8      2           0.725           1.0   \n",
      "3    4        0.8        1.0      4           0.875           1.0   \n",
      "\n",
      "   calibration_error  \n",
      "0              0.125  \n",
      "1              0.250  \n",
      "2              0.275  \n",
      "3              0.125  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, log_loss, accuracy_score\n",
    "import xgboost as xgb\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SECTION 1: DATA LOADING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "defensive_df      = pd.read_csv(os.path.join(\"data/defensive_2019_2023.csv\"))\n",
    "receiving_df      = pd.read_csv(os.path.join(\"data/receiving_2019_2023.csv\"))\n",
    "rushing_df        = pd.read_csv(os.path.join(\"data/rushing_2019_2023.csv\"))\n",
    "passing_df        = pd.read_csv(os.path.join(\"data/passing_2019_2023.csv\"))\n",
    "fumbles_df        = pd.read_csv(os.path.join(\"data/fumbles_2019_2023.csv\"))\n",
    "interceptions_df  = pd.read_csv(os.path.join(\"data/interceptions_2019_2023.csv\"))\n",
    "kickreturns_df    = pd.read_csv(os.path.join(\"data/kickreturns_2019_2023.csv\"))\n",
    "puntreturns_df    = pd.read_csv(os.path.join(\"data/puntreturn_2019_2023.csv\"))\n",
    "kicking_df        = pd.read_csv(os.path.join(\"data/kicking_2019_2023.csv\"))\n",
    "punting_df        = pd.read_csv(os.path.join(\"data/punting_2019_2023.csv\"))\n",
    "\n",
    "print(f\"Loaded 10 datasets\")\n",
    "print(f\"   Receiving: {len(receiving_df)} rows, {len(receiving_df.columns)} cols\")\n",
    "print(f\"   Rushing:   {len(rushing_df)} rows, {len(rushing_df.columns)} cols\")\n",
    "print(f\"   Passing:   {len(passing_df)} rows, {len(passing_df.columns)} cols\")\n",
    "print(f\"   Defensive: {len(defensive_df)} rows, {len(defensive_df.columns)} cols\")\n",
    "print(f\"\\nReceiving columns: {list(receiving_df.columns[:10])}\")\n",
    "print(f\"\\nSample receiving data:\")\n",
    "print(receiving_df.head(3))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 2: ECE CALCULATION FUNCTIONS\")\n",
    "print(\"=\"*70)\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_ece(y_true, y_pred_proba, n_bins=10):\n",
    "    \"\"\"\n",
    "    Calculate Expected Calibration Error\n",
    "    \n",
    "    Args:\n",
    "        y_true: True binary labels (0 or 1)\n",
    "        y_pred_proba: Predicted probabilities [0, 1]\n",
    "        n_bins: Number of bins for calibration (default 10)\n",
    "    \n",
    "    Returns:\n",
    "        ece: Expected Calibration Error\n",
    "        bin_data: DataFrame with per-bin statistics\n",
    "    \"\"\"\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(y_pred_proba, bins) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n",
    "    \n",
    "    ece = 0.0\n",
    "    bin_data = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        bin_mask = bin_indices == i\n",
    "        \n",
    "        if bin_mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        bin_size = bin_mask.sum()\n",
    "        bin_confidence = y_pred_proba[bin_mask].mean()\n",
    "        bin_accuracy = y_true[bin_mask].mean()\n",
    "        \n",
    "        ece += (bin_size / len(y_true)) * abs(bin_confidence - bin_accuracy)\n",
    "        \n",
    "        bin_data.append({\n",
    "            'bin': i,\n",
    "            'bin_lower': bins[i],\n",
    "            'bin_upper': bins[i + 1],\n",
    "            'count': int(bin_size),\n",
    "            'avg_confidence': bin_confidence,\n",
    "            'avg_accuracy': bin_accuracy,\n",
    "            'calibration_error': abs(bin_confidence - bin_accuracy)\n",
    "        })\n",
    "    \n",
    "    return ece, pd.DataFrame(bin_data)\n",
    "\n",
    "\n",
    "def plot_calibration_curve(y_true, y_pred_proba, n_bins=10, title=\"Calibration Curve\"):\n",
    "    \"\"\"Plot calibration curve\"\"\"\n",
    "    ece, bin_data = calculate_ece(y_true, y_pred_proba, n_bins)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    if not bin_data.empty:\n",
    "        ax.plot(bin_data['avg_confidence'], bin_data['avg_accuracy'], \n",
    "                'o-', label=f'Model (ECE={ece:.4f})')\n",
    "    \n",
    "    ax.set_xlabel('Predicted Probability')\n",
    "    ax.set_ylabel('Actual Frequency')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    return fig, ece\n",
    "\n",
    "# Test ECE function\n",
    "print(\"Testing ECE calculation...\")\n",
    "y_test = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 1])\n",
    "y_pred_test = np.array([0.9, 0.1, 0.8, 0.7, 0.3, 0.85, 0.2, 0.15, 0.75, 0.95])\n",
    "test_ece, test_bins = calculate_ece(y_test, y_pred_test, n_bins=5)\n",
    "print(f\"ECE function works! Test ECE: {test_ece:.4f}\")\n",
    "print(f\"\\n Test bins:\\n{test_bins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad1dd910-9ac9-4f2d-b92c-be2693c4a7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 3: DATA PREPARATION FUNCTIONS\n",
      "======================================================================\n",
      "Testing data preparation on receiving data...\n",
      "‚úÖ Data preparation works!\n",
      "   Input rows: 1000, Output rows: 1000\n",
      "   Number of features: 25\n",
      "   Features: ['team_encoded', 'opposing_team_encoded', 'position_encoded', 'home_away_encoded', 'season', 'yds_rolling_3', 'yds_rolling_5', 'yds_rolling_8', 'rec_rolling_3', 'rec_rolling_8']...\n",
      "   Prop outcome distribution: {0: 764, 1: 236}\n",
      "\n",
      "Sample prepared data:\n",
      "       display_name  YDS  prop_outcome  yds_rolling_3  yds_season_avg\n",
      "0      Jason Witten   15             0       0.000000        0.000000\n",
      "1      Jason Witten   25             0      15.000000       15.000000\n",
      "2      Jason Witten   54             1      20.000000       20.000000\n",
      "3      Jason Witten   50             0      31.333333       31.333333\n",
      "4  Larry Fitzgerald  113             1       0.000000        0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 3: DATA PREPARATION FUNCTIONS\")\n",
    "print(\"=\"*70)\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_player_prop_data(df, stat_col='YDS', prop_threshold=50, additional_stat_cols=None):\n",
    "    \"\"\"Prepare data for binary prop prediction\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(['athlete_id', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    if additional_stat_cols is None:\n",
    "        exclude_cols = ['game_id', 'date', 'season', 'team', 'home_away', \n",
    "                       'opposing_team', 'athlete_id', 'display_name', 'position', 'position_abbr']\n",
    "        additional_stat_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    for col in additional_stat_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    df = df.dropna(subset=[stat_col])\n",
    "    df['prop_outcome'] = (df[stat_col] > prop_threshold).astype(int)\n",
    "    \n",
    "    le_team = LabelEncoder()\n",
    "    le_opp = LabelEncoder()\n",
    "    le_position = LabelEncoder()\n",
    "    \n",
    "    df['team_encoded'] = le_team.fit_transform(df['team'].fillna('UNK'))\n",
    "    df['opposing_team_encoded'] = le_opp.fit_transform(df['opposing_team'].fillna('UNK'))\n",
    "    df['position_encoded'] = le_position.fit_transform(df['position'].fillna('UNK'))\n",
    "    df['home_away_encoded'] = (df['home_away'] == 'home').astype(int)\n",
    "    \n",
    "    feature_cols = ['team_encoded', 'opposing_team_encoded', 'position_encoded', \n",
    "                    'home_away_encoded', 'season']\n",
    "    \n",
    "    # Rolling features\n",
    "    for window in [3, 5, 8]:\n",
    "        col_name = f'{stat_col.lower()}_rolling_{window}'\n",
    "        df[col_name] = df.groupby('athlete_id')[stat_col].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        feature_cols.append(col_name)\n",
    "    \n",
    "    for add_stat in additional_stat_cols:\n",
    "        if add_stat != stat_col and add_stat in df.columns:\n",
    "            for window in [3, 8]:\n",
    "                col_name = f'{add_stat.lower()}_rolling_{window}'\n",
    "                df[col_name] = df.groupby('athlete_id')[add_stat].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).mean().shift(1)\n",
    "                )\n",
    "                feature_cols.append(col_name)\n",
    "    \n",
    "    df['prop_hit_rate_8'] = df.groupby('athlete_id')['prop_outcome'].transform(\n",
    "        lambda x: x.rolling(8, min_periods=1).mean().shift(1)\n",
    "    )\n",
    "    feature_cols.append('prop_hit_rate_8')\n",
    "    \n",
    "    df[f'{stat_col.lower()}_season_avg'] = df.groupby(['athlete_id', 'season'])[stat_col].transform(\n",
    "        lambda x: x.expanding().mean().shift(1)\n",
    "    )\n",
    "    feature_cols.append(f'{stat_col.lower()}_season_avg')\n",
    "    \n",
    "    df['prop_season_hit_rate'] = df.groupby(['athlete_id', 'season'])['prop_outcome'].transform(\n",
    "        lambda x: x.expanding().mean().shift(1)\n",
    "    )\n",
    "    feature_cols.append('prop_season_hit_rate')\n",
    "    \n",
    "    df[f'{stat_col.lower()}_std_8'] = df.groupby('athlete_id')[stat_col].transform(\n",
    "        lambda x: x.rolling(8, min_periods=2).std().shift(1)\n",
    "    )\n",
    "    feature_cols.append(f'{stat_col.lower()}_std_8')\n",
    "    \n",
    "    df['trend'] = df[f'{stat_col.lower()}_rolling_3'] - df[f'{stat_col.lower()}_season_avg']\n",
    "    feature_cols.append('trend')\n",
    "    \n",
    "    df['game_num'] = df.groupby(['athlete_id', 'season']).cumcount() + 1\n",
    "    feature_cols.append('game_num')\n",
    "    \n",
    "    df['career_games'] = df.groupby('athlete_id').cumcount()\n",
    "    feature_cols.append('career_games')\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    feature_cols = list(dict.fromkeys(feature_cols))\n",
    "    \n",
    "    return df, feature_cols, le_team, le_opp, le_position\n",
    "\n",
    "# Test data preparation\n",
    "print(\"Testing data preparation on receiving data...\")\n",
    "test_prepared, test_features, _, _, _ = prepare_player_prop_data(\n",
    "    receiving_df.head(1000), \n",
    "    stat_col='YDS', \n",
    "    prop_threshold=50\n",
    ")\n",
    "print(f\"Data preparation works!\")\n",
    "print(f\"Input rows: 1000, Output rows: {len(test_prepared)}\")\n",
    "print(f\"Number of features: {len(test_features)}\")\n",
    "print(f\"Features: {test_features[:10]}...\")\n",
    "print(f\"Prop outcome distribution: {test_prepared['prop_outcome'].value_counts().to_dict()}\")\n",
    "print(f\"\\nSample prepared data:\")\n",
    "print(test_prepared[['display_name', 'YDS', 'prop_outcome', 'yds_rolling_3', 'yds_season_avg']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9399dd2-dc37-451c-8e64-46769bb53db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 4: MODEL TRAINING FUNCTION\n",
      "======================================================================\n",
      "Testing single model training on small subset...\n",
      "\n",
      "======================================================================\n",
      "Training: receiving_test - YDS Over 50\n",
      "======================================================================\n",
      "Training samples: 1600, Test samples: 400\n",
      "Train hit rate: 23.56%, Test hit rate: 18.00%\n",
      "\n",
      "üìä TRAIN | Log Loss: 0.2365 | AUC: 0.9721 | Acc: 0.9244 | ECE: 0.1001\n",
      "üìä TEST  | Log Loss: 0.4255 | AUC: 0.7328 | Acc: 0.8100 | ECE: 0.0453\n",
      "‚úÖ Well calibrated: ECE < 0.10\n",
      "‚úÖ Task validated: AUC > 0.60\n",
      "\n",
      "üîù Top 5 Features:\n",
      "   yds_rolling_8: 4.74\n",
      "   tgts_rolling_8: 4.50\n",
      "   yds_season_avg: 4.00\n",
      "   tgts_rolling_3: 3.90\n",
      "   position_encoded: 3.57\n",
      "\n",
      "‚úÖ Single model training works!\n",
      "   Model type: <class 'xgboost.core.Booster'>\n",
      "   Test AUC: 0.7328\n",
      "   Test ECE: 0.0453\n",
      "\n",
      "======================================================================\n",
      "SECTION 5: INFERENCE FUNCTION\n",
      "======================================================================\n",
      "Testing inference function...\n",
      "Testing with player: Larry Fitzgerald\n",
      "   Player has 9 games in prepared data\n",
      "‚úÖ Inference function works!\n",
      "   Test player: Larry Fitzgerald\n",
      "   Prediction:\n",
      "      player: Larry Fitzgerald\n",
      "      prop: YDS Over 50\n",
      "      prob_over: 0.1135\n",
      "      prob_under: 0.8865\n",
      "      recommendation: BET UNDER\n",
      "      confidence: 0.8865\n",
      "      recent_avg: 29.6667\n",
      "      season_avg: 55.8750\n",
      "      model_ece: 0.0453\n",
      "      model_auc: 0.7328\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 4: MODEL TRAINING FUNCTION\")\n",
    "print(\"=\"*70)\n",
    "# ============================================================================\n",
    "\n",
    "def train_prop_model(df, stat_col, threshold, category_name, \n",
    "                     additional_stats=None, params=None):\n",
    "    \"\"\"Train prop model with ECE evaluation\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training: {category_name} - {stat_col} Over {threshold}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    prepared_df, feature_cols, le_team, le_opp, le_position = prepare_player_prop_data(\n",
    "        df, stat_col, threshold, additional_stats\n",
    "    )\n",
    "    \n",
    "    if len(prepared_df) < 100:\n",
    "        print(f\"Insufficient data: {len(prepared_df)} samples. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    X = prepared_df[feature_cols].values\n",
    "    y = prepared_df['prop_outcome'].values\n",
    "    \n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "    print(f\"Train hit rate: {y_train.mean():.2%}, Test hit rate: {y_test.mean():.2%}\")\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    if params is None:\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'eval_metric': ['logloss', 'auc'],\n",
    "            'seed': 42,\n",
    "            'scale_pos_weight': 1\n",
    "        }\n",
    "    \n",
    "    evals = [(dtrain, 'train'), (dtest, 'test')]\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    y_pred_train_proba = model.predict(dtrain)\n",
    "    y_pred_test_proba = model.predict(dtest)\n",
    "    y_pred_train = (y_pred_train_proba > 0.5).astype(int)\n",
    "    y_pred_test = (y_pred_test_proba > 0.5).astype(int)\n",
    "    \n",
    "    train_ece, train_bin_data = calculate_ece(y_train, y_pred_train_proba)\n",
    "    test_ece, test_bin_data = calculate_ece(y_test, y_pred_test_proba)\n",
    "    \n",
    "    train_metrics = {\n",
    "        'log_loss': log_loss(y_train, y_pred_train_proba),\n",
    "        'roc_auc': roc_auc_score(y_train, y_pred_train_proba),\n",
    "        'accuracy': accuracy_score(y_train, y_pred_train),\n",
    "        'ece': train_ece\n",
    "    }\n",
    "    \n",
    "    test_metrics = {\n",
    "        'log_loss': log_loss(y_test, y_pred_test_proba),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_test_proba),\n",
    "        'accuracy': accuracy_score(y_test, y_pred_test),\n",
    "        'ece': test_ece\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTRAIN | Log Loss: {train_metrics['log_loss']:.4f} | \"\n",
    "          f\"AUC: {train_metrics['roc_auc']:.4f} | Acc: {train_metrics['accuracy']:.4f} | \"\n",
    "          f\"ECE: {train_metrics['ece']:.4f}\")\n",
    "    print(f\"TEST  | Log Loss: {test_metrics['log_loss']:.4f} | \"\n",
    "          f\"AUC: {test_metrics['roc_auc']:.4f} | Acc: {test_metrics['accuracy']:.4f} | \"\n",
    "          f\"ECE: {test_metrics['ece']:.4f}\")\n",
    "    \n",
    "    if test_metrics['ece'] < 0.10:\n",
    "        print(\"Well calibrated: ECE < 0.10\")\n",
    "    elif test_metrics['ece'] < 0.15:\n",
    "        print(\"Moderate calibration: 0.10 < ECE < 0.15\")\n",
    "    else:\n",
    "        print(\"Poor calibration: ECE > 0.15\")\n",
    "    \n",
    "    if test_metrics['roc_auc'] > 0.60:\n",
    "        print(\"Task validated: AUC > 0.60\")\n",
    "    else:\n",
    "        print(\"Warning: AUC < 0.60\")\n",
    "    \n",
    "    importance = model.get_score(importance_type='gain')\n",
    "    importance_df = pd.DataFrame([\n",
    "        {'feature': feature_cols[int(k[1:])], 'gain': v}\n",
    "        for k, v in importance.items()\n",
    "    ]).sort_values('gain', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüîù Top 5 Features:\")\n",
    "    for idx, row in importance_df.head(5).iterrows():\n",
    "        print(f\"   {row['feature']}: {row['gain']:.2f}\")\n",
    "    \n",
    "    fig, _ = plot_calibration_curve(\n",
    "        y_test, y_pred_test_proba, \n",
    "        title=f\"{category_name} {stat_col} O{threshold} - Calibration\"\n",
    "    )\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    fig.savefig(f\"plots/calibration_{category_name}_{stat_col}_{threshold}.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'feature_cols': feature_cols,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'importance': importance_df,\n",
    "        'calibration': {\n",
    "            'train_bins': train_bin_data,\n",
    "            'test_bins': test_bin_data\n",
    "        },\n",
    "        'encoders': {\n",
    "            'team': le_team,\n",
    "            'opponent': le_opp,\n",
    "            'position': le_position\n",
    "        },\n",
    "        'prepared_df': prepared_df,\n",
    "        'metadata': {\n",
    "            'category': category_name,\n",
    "            'stat': stat_col,\n",
    "            'threshold': threshold,\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test),\n",
    "            'train_hit_rate': float(y_train.mean()),\n",
    "            'test_hit_rate': float(y_test.mean())\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test single model training\n",
    "print(\"Testing single model training on small subset...\")\n",
    "test_model_result = train_prop_model(\n",
    "    df=receiving_df.head(2000),\n",
    "    stat_col='YDS',\n",
    "    threshold=50,\n",
    "    category_name='receiving_test',\n",
    "    additional_stats=['REC', 'YDS', 'TD', 'TGTS']\n",
    ")\n",
    "\n",
    "if test_model_result:\n",
    "    print(f\"\\nSingle model training works!\")\n",
    "    print(f\"   Model type: {type(test_model_result['model'])}\")\n",
    "    print(f\"   Test AUC: {test_model_result['test_metrics']['roc_auc']:.4f}\")\n",
    "    print(f\"   Test ECE: {test_model_result['test_metrics']['ece']:.4f}\")\n",
    "else:\n",
    "    print(\"Single model training failed\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 5: INFERENCE FUNCTION\")\n",
    "print(\"=\"*70)\n",
    "# ============================================================================\n",
    "\n",
    "def predict_player_prop(player_name, stat, threshold, trained_models, \n",
    "                        receiving_df, rushing_df, passing_df):\n",
    "    \"\"\"\n",
    "    Predict probability for a specific player prop\n",
    "    \n",
    "    Args:\n",
    "        player_name: \"Xavier Worthy\"\n",
    "        stat: \"YDS\"\n",
    "        threshold: 65\n",
    "        trained_models: Dictionary of trained models\n",
    "        receiving_df, rushing_df, passing_df: DataFrames\n",
    "    \n",
    "    Returns:\n",
    "        dict with probabilities and recommendation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine category from stat\n",
    "    if stat in ['REC', 'TGTS']:\n",
    "        category = 'receiving'\n",
    "        df = receiving_df\n",
    "    elif stat == 'YDS':\n",
    "        category = 'receiving'  # Default to receiving, could be improved\n",
    "        df = receiving_df\n",
    "    elif stat in ['CAR']:\n",
    "        category = 'rushing'\n",
    "        df = rushing_df\n",
    "    elif stat in ['TD', 'INT']:\n",
    "        category = 'passing'\n",
    "        df = passing_df\n",
    "    else:\n",
    "        return {\"error\": \"Cannot determine category from stat\"}\n",
    "    \n",
    "    # Build prop name\n",
    "    prop_name = f\"{category}_{stat}_over_{threshold}\"\n",
    "    if prop_name not in trained_models:\n",
    "        return {\"error\": f\"No trained model for {prop_name}. Available models: {list(trained_models.keys())}\"}\n",
    "    \n",
    "    model_data = trained_models[prop_name]\n",
    "    model = model_data['model']\n",
    "    feature_cols = model_data['feature_cols']\n",
    "    prepared_df = model_data['prepared_df']\n",
    "    \n",
    "    # Find player in prepared data\n",
    "    player_data = prepared_df[prepared_df['display_name'] == player_name].copy()\n",
    "    \n",
    "    if player_data.empty:\n",
    "        return {\"error\": f\"Player '{player_name}' not found in {category} data\"}\n",
    "    \n",
    "    # Get most recent game\n",
    "    latest_game = player_data.iloc[-1]\n",
    "    \n",
    "    # Prepare input features\n",
    "    X_input = latest_game[feature_cols].values.reshape(1, -1)\n",
    "    dmatrix = xgb.DMatrix(X_input, feature_names=feature_cols)\n",
    "    \n",
    "    # Predict\n",
    "    prob_over = model.predict(dmatrix)[0]\n",
    "    prob_under = 1 - prob_over\n",
    "    \n",
    "    return {\n",
    "        'player': player_name,\n",
    "        'prop': f\"{stat} Over {threshold}\",\n",
    "        'prob_over': float(prob_over),\n",
    "        'prob_under': float(prob_under),\n",
    "        'recommendation': 'BET OVER' if prob_over > 0.55 else 'BET UNDER' if prob_under > 0.55 else 'NO BET',\n",
    "        'confidence': float(max(prob_over, prob_under)),\n",
    "        'recent_avg': float(latest_game.get(f'{stat.lower()}_rolling_3', 0)),\n",
    "        'season_avg': float(latest_game.get(f'{stat.lower()}_season_avg', 0)),\n",
    "        'model_ece': float(model_data['test_metrics']['ece']),\n",
    "        'model_auc': float(model_data['test_metrics']['roc_auc'])\n",
    "    }\n",
    "\n",
    "\n",
    "# Test inference with the test model we just trained\n",
    "print(\"Testing inference function...\")\n",
    "if test_model_result:\n",
    "    # Get a sample player name from the prepared data (not raw receiving_df)\n",
    "    prepared_df = test_model_result['prepared_df']\n",
    "    \n",
    "    # Find a player with multiple games\n",
    "    player_counts = prepared_df['display_name'].value_counts()\n",
    "    sample_player = player_counts[player_counts > 5].index[0] if len(player_counts[player_counts > 5]) > 0 else player_counts.index[0]\n",
    "    \n",
    "    print(f\"Testing with player: {sample_player}\")\n",
    "    print(f\"   Player has {player_counts[sample_player]} games in prepared data\")\n",
    "    \n",
    "    # Create a minimal trained_models dict for testing\n",
    "    test_trained_models = {\n",
    "        'receiving_YDS_over_50': test_model_result\n",
    "    }\n",
    "    \n",
    "    # Test the prediction\n",
    "    test_prediction = predict_player_prop(\n",
    "        player_name=sample_player,\n",
    "        stat='YDS',\n",
    "        threshold=50,\n",
    "        trained_models=test_trained_models,\n",
    "        receiving_df=receiving_df,\n",
    "        rushing_df=rushing_df,\n",
    "        passing_df=passing_df\n",
    "    )\n",
    "    \n",
    "    if 'error' in test_prediction:\n",
    "        print(f\"Inference failed: {test_prediction['error']}\")\n",
    "    else:\n",
    "        print(f\"Inference function works!\")\n",
    "        print(f\"Test player: {sample_player}\")\n",
    "        print(f\"Prediction:\")\n",
    "        for key, value in test_prediction.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"      {key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"      {key}: {value}\")\n",
    "else:\n",
    "    print(\"Skipping inference test (model training failed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cf31d90-6cdc-43be-84d6-b0b02956fe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 6: MULTI-PROP TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "‚úÖ Configuration loaded\n",
      "   Total props to train: 8\n",
      "\n",
      "Props breakdown:\n",
      "   1. receiving - YDS Over 50\n",
      "   2. receiving - YDS Over 65\n",
      "   3. receiving - YDS Over 75\n",
      "   4. receiving - REC Over 5\n",
      "   5. rushing - YDS Over 50\n",
      "   6. rushing - YDS Over 75\n",
      "   7. passing - YDS Over 250\n",
      "   8. passing - TD Over 1.5\n",
      "\n",
      "‚ö†Ô∏è  Ready to start full training loop!\n",
      "   This will train all models and may take several minutes.\n",
      "   Proceed to Section 7 to start training.\n",
      "\n",
      "======================================================================\n",
      "SECTION 7: FULL MULTI-PROP TRAINING LOOP\n",
      "======================================================================\n",
      "Uncomment the code below to run full training\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Training: receiving - YDS Over 50\n",
      "======================================================================\n",
      "Training samples: 17929, Test samples: 4483\n",
      "Train hit rate: 20.43%, Test hit rate: 20.72%\n",
      "\n",
      "üìä TRAIN | Log Loss: 0.3128 | AUC: 0.9049 | Acc: 0.8692 | ECE: 0.0441\n",
      "üìä TEST  | Log Loss: 0.4129 | AUC: 0.8059 | Acc: 0.8099 | ECE: 0.0373\n",
      "‚úÖ Well calibrated: ECE < 0.10\n",
      "‚úÖ Task validated: AUC > 0.60\n",
      "\n",
      "üîù Top 5 Features:\n",
      "   yds_rolling_8: 44.41\n",
      "   tgts_rolling_8: 14.41\n",
      "   position_encoded: 12.38\n",
      "   yds_rolling_5: 9.32\n",
      "   tgts_rolling_3: 6.13\n",
      "\n",
      "======================================================================\n",
      "Training: receiving - YDS Over 65\n",
      "======================================================================\n",
      "Training samples: 17929, Test samples: 4483\n",
      "Train hit rate: 12.89%, Test hit rate: 13.52%\n",
      "\n",
      "üìä TRAIN | Log Loss: 0.2286 | AUC: 0.9238 | Acc: 0.9099 | ECE: 0.0344\n",
      "üìä TEST  | Log Loss: 0.3148 | AUC: 0.8253 | Acc: 0.8659 | ECE: 0.0278\n",
      "‚úÖ Well calibrated: ECE < 0.10\n",
      "‚úÖ Task validated: AUC > 0.60\n",
      "\n",
      "üîù Top 5 Features:\n",
      "   yds_rolling_8: 45.61\n",
      "   position_encoded: 11.17\n",
      "   tgts_rolling_8: 9.93\n",
      "   yds_rolling_5: 8.74\n",
      "   yds_season_avg: 5.87\n",
      "\n",
      "======================================================================\n",
      "Training: receiving - YDS Over 75\n",
      "======================================================================\n",
      "Training samples: 17929, Test samples: 4483\n",
      "Train hit rate: 9.56%, Test hit rate: 10.42%\n",
      "\n",
      "üìä TRAIN | Log Loss: 0.1643 | AUC: 0.9561 | Acc: 0.9340 | ECE: 0.0446\n",
      "üìä TEST  | Log Loss: 0.2643 | AUC: 0.8413 | Acc: 0.8954 | ECE: 0.0293\n",
      "‚úÖ Well calibrated: ECE < 0.10\n",
      "‚úÖ Task validated: AUC > 0.60\n",
      "\n",
      "üîù Top 5 Features:\n",
      "   yds_rolling_8: 31.21\n",
      "   position_encoded: 9.22\n",
      "   tgts_rolling_8: 8.57\n",
      "   yds_rolling_5: 6.59\n",
      "   yds_season_avg: 4.67\n",
      "\n",
      "======================================================================\n",
      "Training: receiving - REC Over 5\n",
      "======================================================================\n",
      "Training samples: 17929, Test samples: 4483\n",
      "Train hit rate: 12.41%, Test hit rate: 11.98%\n",
      "\n",
      "üìä TRAIN | Log Loss: 0.2309 | AUC: 0.9126 | Acc: 0.9074 | ECE: 0.0314\n",
      "üìä TEST  | Log Loss: 0.2900 | AUC: 0.8242 | Acc: 0.8860 | ECE: 0.0177\n",
      "‚úÖ Well calibrated: ECE < 0.10\n",
      "‚úÖ Task validated: AUC > 0.60\n",
      "\n",
      "üîù Top 5 Features:\n",
      "   tgts_rolling_8: 47.27\n",
      "   rec_rolling_8: 40.36\n",
      "   rec_rolling_5: 11.22\n",
      "   rec_season_avg: 9.13\n",
      "   rec_rolling_3: 5.61\n",
      "\n",
      "======================================================================\n",
      "Training: rushing - YDS Over 50\n",
      "======================================================================\n",
      "Training samples: 9227, Test samples: 2307\n",
      "Train hit rate: 19.43%, Test hit rate: 20.81%\n",
      "\n",
      "üìä TRAIN | Log Loss: 0.2239 | AUC: 0.9626 | Acc: 0.9160 | ECE: 0.0624\n",
      "üìä TEST  | Log Loss: 0.3701 | AUC: 0.8550 | Acc: 0.8123 | ECE: 0.0419\n",
      "‚úÖ Well calibrated: ECE < 0.10\n",
      "‚úÖ Task validated: AUC > 0.60\n",
      "\n",
      "üîù Top 5 Features:\n",
      "   yds_rolling_5: 21.57\n",
      "   car_rolling_3: 13.96\n",
      "   car_rolling_8: 12.08\n",
      "   position_encoded: 4.88\n",
      "   yds_rolling_8: 4.65\n",
      "\n",
      "======================================================================\n",
      "Training: rushing - YDS Over 75\n",
      "======================================================================\n",
      "Training samples: 9227, Test samples: 2307\n",
      "Train hit rate: 9.05%, Test hit rate: 9.93%\n",
      "\n",
      "üìä TRAIN | Log Loss: 0.1272 | AUC: 0.9815 | Acc: 0.9517 | ECE: 0.0524\n",
      "üìä TEST  | Log Loss: 0.2531 | AUC: 0.8494 | Acc: 0.9007 | ECE: 0.0273\n",
      "‚úÖ Well calibrated: ECE < 0.10\n",
      "‚úÖ Task validated: AUC > 0.60\n",
      "\n",
      "üîù Top 5 Features:\n",
      "   car_rolling_3: 16.72\n",
      "   yds_rolling_5: 12.15\n",
      "   yds_rolling_8: 6.56\n",
      "   car_rolling_8: 5.79\n",
      "   yds_season_avg: 3.57\n",
      "\n",
      "======================================================================\n",
      "Training: passing - YDS Over 250\n",
      "======================================================================\n",
      "Training samples: 2760, Test samples: 690\n",
      "Train hit rate: 35.00%, Test hit rate: 29.86%\n",
      "\n",
      "üìä TRAIN | Log Loss: 0.3008 | AUC: 0.9830 | Acc: 0.9388 | ECE: 0.1675\n",
      "üìä TEST  | Log Loss: 0.5746 | AUC: 0.6911 | Acc: 0.6913 | ECE: 0.0763\n",
      "‚úÖ Well calibrated: ECE < 0.10\n",
      "‚úÖ Task validated: AUC > 0.60\n",
      "\n",
      "üîù Top 5 Features:\n",
      "   yds_rolling_5: 7.12\n",
      "   yds_rolling_3: 4.89\n",
      "   yds_rolling_8: 3.97\n",
      "   yds_season_avg: 3.55\n",
      "   td_rolling_3: 3.34\n",
      "\n",
      "======================================================================\n",
      "Training: passing - TD Over 1.5\n",
      "======================================================================\n",
      "Training samples: 2760, Test samples: 690\n",
      "Train hit rate: 38.08%, Test hit rate: 30.29%\n",
      "\n",
      "üìä TRAIN | Log Loss: 0.3167 | AUC: 0.9783 | Acc: 0.9301 | ECE: 0.1686\n",
      "üìä TEST  | Log Loss: 0.6112 | AUC: 0.6573 | Acc: 0.6507 | ECE: 0.0912\n",
      "‚úÖ Well calibrated: ECE < 0.10\n",
      "‚úÖ Task validated: AUC > 0.60\n",
      "\n",
      "üîù Top 5 Features:\n",
      "   yds_rolling_8: 6.30\n",
      "   yds_rolling_3: 4.53\n",
      "   prop_season_hit_rate: 3.97\n",
      "   td_rolling_8: 3.60\n",
      "   season: 3.03\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE - SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Total models trained: 8\n",
      "Models with AUC > 0.60: 8\n",
      "Models with ECE < 0.10: 8\n",
      "Average Test AUC: 0.7937\n",
      "Average Test ECE: 0.0436\n",
      "\n",
      "======================================================================\n",
      "BEST CALIBRATED MODELS (Lowest ECE)\n",
      "======================================================================\n",
      "                 prop  test_ece  test_auc  test_accuracy\n",
      " receiving_REC_over_5  0.017744  0.824179       0.886014\n",
      "  rushing_YDS_over_75  0.027273  0.849402       0.900737\n",
      "receiving_YDS_over_65  0.027750  0.825273       0.865938\n",
      "receiving_YDS_over_75  0.029257  0.841291       0.895383\n",
      "receiving_YDS_over_50  0.037275  0.805884       0.809949\n",
      "  rushing_YDS_over_50  0.041926  0.855040       0.812310\n",
      " passing_YDS_over_250  0.076270  0.691051       0.691304\n",
      "  passing_TD_over_1.5  0.091232  0.657303       0.650725\n",
      "\n",
      "======================================================================\n",
      "BEST DISCRIMINATIVE MODELS (Highest AUC)\n",
      "======================================================================\n",
      "                 prop  test_auc  test_ece  test_accuracy\n",
      "  rushing_YDS_over_50  0.855040  0.041926       0.812310\n",
      "  rushing_YDS_over_75  0.849402  0.027273       0.900737\n",
      "receiving_YDS_over_75  0.841291  0.029257       0.895383\n",
      "receiving_YDS_over_65  0.825273  0.027750       0.865938\n",
      " receiving_REC_over_5  0.824179  0.017744       0.886014\n",
      "receiving_YDS_over_50  0.805884  0.037275       0.809949\n",
      " passing_YDS_over_250  0.691051  0.076270       0.691304\n",
      "  passing_TD_over_1.5  0.657303  0.091232       0.650725\n",
      "\n",
      "‚úÖ Summary saved to models/training_summary_with_ece.csv\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE PREDICTIONS\n",
      "======================================================================\n",
      "Sample players available: ['Stefon Diggs', 'Travis Kelce', 'Tyler Boyd', 'Tyreek Hill', 'Mike Evans']\n",
      "\n",
      "Stefon Diggs:\n",
      "{\n",
      "  \"player\": \"Stefon Diggs\",\n",
      "  \"prop\": \"YDS Over 65\",\n",
      "  \"prob_over\": 0.3574868142604828,\n",
      "  \"prob_under\": 0.6425131559371948,\n",
      "  \"recommendation\": \"BET UNDER\",\n",
      "  \"confidence\": 0.6425131559371948,\n",
      "  \"recent_avg\": 55.0,\n",
      "  \"season_avg\": 68.61111111111111,\n",
      "  \"model_ece\": 0.027750020341487727,\n",
      "  \"model_auc\": 0.8252727645733365\n",
      "}\n",
      "\n",
      "Travis Kelce:\n",
      "{\n",
      "  \"player\": \"Travis Kelce\",\n",
      "  \"prop\": \"YDS Over 65\",\n",
      "  \"prob_over\": 0.596847414970398,\n",
      "  \"prob_under\": 0.40315258502960205,\n",
      "  \"recommendation\": \"BET OVER\",\n",
      "  \"confidence\": 0.596847414970398,\n",
      "  \"recent_avg\": 87.33333333333333,\n",
      "  \"season_avg\": 71.3125,\n",
      "  \"model_ece\": 0.027750020341487727,\n",
      "  \"model_auc\": 0.8252727645733365\n",
      "}\n",
      "\n",
      "======================================================================\n",
      "ALL TESTING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "‚úÖ All sections loaded successfully!\n",
      "   To run full training, uncomment Section 7 code block above.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 6: MULTI-PROP TRAINING CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "# ============================================================================\n",
    "\n",
    "PROPS_CONFIG = [\n",
    "    ('receiving', receiving_df, 'YDS', 50, ['REC', 'YDS', 'TD', 'TGTS']),\n",
    "    ('receiving', receiving_df, 'YDS', 65, ['REC', 'YDS', 'TD', 'TGTS']),\n",
    "    ('receiving', receiving_df, 'YDS', 75, ['REC', 'YDS', 'TD', 'TGTS']),\n",
    "    ('receiving', receiving_df, 'REC', 5, ['REC', 'YDS', 'TD', 'TGTS']),\n",
    "    ('rushing', rushing_df, 'YDS', 50, ['CAR', 'YDS', 'TD']),\n",
    "    ('rushing', rushing_df, 'YDS', 75, ['CAR', 'YDS', 'TD']),\n",
    "    ('passing', passing_df, 'YDS', 250, ['YDS', 'TD', 'INT']),\n",
    "    ('passing', passing_df, 'TD', 1.5, ['YDS', 'TD', 'INT']),\n",
    "]\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"   Total props to train: {len(PROPS_CONFIG)}\")\n",
    "print(f\"\\nProps breakdown:\")\n",
    "for i, (cat, _, stat, thresh, _) in enumerate(PROPS_CONFIG, 1):\n",
    "    print(f\"   {i}. {cat} - {stat} Over {thresh}\")\n",
    "\n",
    "print(\"\\nReady to start full training loop!\")\n",
    "print(\"   This will train all models and may take several minutes.\")\n",
    "print(\"   Proceed to Section 7 to start training.\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 7: FULL MULTI-PROP TRAINING LOOP\")\n",
    "print(\"=\"*70)\n",
    "print(\"Uncomment the code below to run full training\")\n",
    "print(\"=\"*70)\n",
    "# ============================================================================\n",
    "\n",
    "# UNCOMMENT TO RUN FULL TRAINING:\n",
    "\n",
    "trained_models = {}\n",
    "results_summary = []\n",
    "\n",
    "for category, df, stat_col, threshold, additional_stats in PROPS_CONFIG:\n",
    "    prop_name = f\"{category}_{stat_col}_over_{threshold}\"\n",
    "    \n",
    "    try:\n",
    "        result = train_prop_model(\n",
    "            df=df,\n",
    "            stat_col=stat_col,\n",
    "            threshold=threshold,\n",
    "            category_name=category,\n",
    "            additional_stats=additional_stats\n",
    "        )\n",
    "        \n",
    "        if result is not None:\n",
    "            trained_models[prop_name] = result\n",
    "            \n",
    "            os.makedirs(\"models\", exist_ok=True)\n",
    "            result['model'].save_model(f\"models/xgb_{prop_name}.json\")\n",
    "            \n",
    "            results_summary.append({\n",
    "                'prop': prop_name,\n",
    "                'category': category,\n",
    "                'stat': stat_col,\n",
    "                'threshold': threshold,\n",
    "                'test_auc': result['test_metrics']['roc_auc'],\n",
    "                'test_accuracy': result['test_metrics']['accuracy'],\n",
    "                'test_logloss': result['test_metrics']['log_loss'],\n",
    "                'test_ece': result['test_metrics']['ece'],\n",
    "                'train_samples': result['metadata']['train_size'],\n",
    "                'test_samples': result['metadata']['test_size']\n",
    "            })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {prop_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: SUMMARY AND RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "\n",
    "results_df_by_ece = results_df.sort_values('test_ece')\n",
    "results_df_by_auc = results_df.sort_values('test_auc', ascending=False)\n",
    "\n",
    "print(f\"\\nTotal models trained: {len(results_df)}\")\n",
    "print(f\"Models with AUC > 0.60: {(results_df['test_auc'] > 0.60).sum()}\")\n",
    "print(f\"Models with ECE < 0.10: {(results_df['test_ece'] < 0.10).sum()}\")\n",
    "print(f\"Average Test AUC: {results_df['test_auc'].mean():.4f}\")\n",
    "print(f\"Average Test ECE: {results_df['test_ece'].mean():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST CALIBRATED MODELS (Lowest ECE)\")\n",
    "print(\"=\"*70)\n",
    "print(results_df_by_ece[['prop', 'test_ece', 'test_auc', 'test_accuracy']].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST DISCRIMINATIVE MODELS (Highest AUC)\")\n",
    "print(\"=\"*70)\n",
    "print(results_df_by_auc[['prop', 'test_auc', 'test_ece', 'test_accuracy']].head(10).to_string(index=False))\n",
    "\n",
    "results_df.to_csv(\"models/training_summary_with_ece.csv\", index=False)\n",
    "print(\"\\n Summary saved to models/training_summary_with_ece.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: EXAMPLE PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find a player that exists in the data\n",
    "sample_players = receiving_df['display_name'].value_counts().head(5).index.tolist()\n",
    "print(f\"Sample players available: {sample_players}\")\n",
    "\n",
    "for player in sample_players[:2]:\n",
    "    prediction = predict_player_prop(\n",
    "        player_name=player,\n",
    "        stat=\"YDS\",\n",
    "        threshold=65,\n",
    "        trained_models=trained_models,\n",
    "        receiving_df=receiving_df,\n",
    "        rushing_df=rushing_df,\n",
    "        passing_df=passing_df\n",
    "    )\n",
    "    print(f\"\\n{player}:\")\n",
    "    print(json.dumps(prediction, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL TESTING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "print(\"\\nAll sections loaded successfully!\")\n",
    "print(\"   To run full training, uncomment Section 7 code block above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4fd4f-ad4c-4ce0-b283-d480b530a9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsvenv",
   "language": "python",
   "name": "dsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
