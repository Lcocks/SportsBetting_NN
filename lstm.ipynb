{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, tarfile, urllib.request, random, re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@dataclass\n",
    "class StatSeqConfig:\n",
    "    n_past_games: int = 5     # window length T\n",
    "    batch_size: int = 64\n",
    "    hidden_size: int = 128\n",
    "    n_epochs: int = 20\n",
    "    lr: float = 1e-3\n",
    "\n",
    "\n",
    "dir = \"data\"\n",
    "\n",
    "stats_list = [\n",
    "    \"rushing\", \"receiving\", \"passing\", \"defensive\",\n",
    "    \"kicking\", \"punting\", \"fumbles\", \"interceptions\",\n",
    "    \"kickreturns\", \"puntreturn\",\n",
    "]\n",
    "\n",
    "def load_stat_df(stat_name: str) -> pd.DataFrame:\n",
    "\n",
    "    if stat_name not in stats_list:\n",
    "        raise ValueError(\n",
    "            \"Please provide a valid stat_name from: \"\n",
    "            + \", \".join(stats_list)\n",
    "        )\n",
    "\n",
    "    train_path = os.path.join(dir, f\"{stat_name}_2019_2023.csv\")\n",
    "    test_path  = os.path.join(dir, f\"{stat_name}_24tocurrent.csv\")\n",
    "\n",
    "    df_train = pd.read_csv(train_path, parse_dates=[\"date\"])\n",
    "    df_test  = pd.read_csv(test_path,  parse_dates=[\"date\"])\n",
    "\n",
    "    df_all = pd.concat([df_train, df_test], ignore_index=True)\n",
    "    df_all = df_all.sort_values([\"athlete_id\", \"date\"]).reset_index(drop=True)\n",
    "    return df_all\n",
    "\n",
    "\n",
    "class StatSequenceDataset(Dataset):\n",
    "    def __init__(self, samples, feature_cols):\n",
    "\n",
    "        self.samples = samples\n",
    "        self.feature_cols = feature_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        x = s[\"x\"]\n",
    "        y = s[\"y\"]\n",
    "\n",
    "        x = torch.from_numpy(x.astype(\"float32\"))\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        length = torch.tensor(x.shape[0], dtype=torch.long)\n",
    "        return x, length, y\n",
    "\n",
    "\n",
    "def make_train_test_sequences(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols,\n",
    "    target_col=\"YDS\",\n",
    "    window=5,\n",
    "):\n",
    "\n",
    "    samples_train = []\n",
    "    samples_test  = []\n",
    "\n",
    "    for athlete_id, g in df.groupby(\"athlete_id\"):\n",
    "        g = g.sort_values(\"date\")\n",
    "        X_feats = g[feature_cols].values.astype(\"float32\")\n",
    "        y_vals  = g[target_col].values.astype(\"float32\")\n",
    "        seasons = g[\"season\"].values\n",
    "\n",
    "        if len(g) <= window:\n",
    "            continue\n",
    "\n",
    "        for t in range(window, len(g)):\n",
    "            x_seq = X_feats[t-window : t]\n",
    "            y     = y_vals[t]\n",
    "            season_t = seasons[t]\n",
    "\n",
    "            sample = {\"x\": x_seq, \"y\": y}\n",
    "\n",
    "            if season_t <= 2023:\n",
    "                samples_train.append(sample)\n",
    "            else:\n",
    "                samples_test.append(sample)\n",
    "\n",
    "    train_ds = StatSequenceDataset(samples_train, feature_cols)\n",
    "    test_ds  = StatSequenceDataset(samples_test,  feature_cols)\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "def stat_collate_fn(batch):\n",
    "\n",
    "    xs, lengths, ys = zip(*batch)\n",
    "    xs = torch.stack(xs, dim=0)\n",
    "    lengths = torch.stack(lengths, 0)\n",
    "    ys = torch.stack(ys, 0)\n",
    "    return xs, lengths, ys\n",
    "\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            nn.init.xavier_uniform_(self.W.weight)\n",
    "            nn.init.zeros_(self.W.bias)\n",
    "            b = self.W.bias.view(4, hidden_size)\n",
    "            b[0].fill_(1.0)  # forget gate bias\n",
    "\n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "\n",
    "        z = self.W(torch.cat([x_t, h_prev], dim=1))\n",
    "        H = self.hidden_size\n",
    "        f, i, o, g = z.chunk(4, dim=1)\n",
    "\n",
    "        f = torch.sigmoid(f)\n",
    "        i = torch.sigmoid(i)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "\n",
    "        c_t = f * c_prev + i * g\n",
    "        h_t = o * torch.tanh(c_t)\n",
    "        return h_t, c_t, (f, i, o, g)\n",
    "\n",
    "\n",
    "class LSTMSequence(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.cell = LSTMCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "\n",
    "        B, T, E = x.shape\n",
    "        H = self.cell.hidden_size\n",
    "\n",
    "        h = x.new_zeros(B, H)\n",
    "        c = x.new_zeros(B, H)\n",
    "\n",
    "        for t in range(T):\n",
    "            mask_t = (lengths > t).float().unsqueeze(1)  # (B, 1)\n",
    "\n",
    "            x_t = x[:, t, :]  # (B, E)\n",
    "            h_new, c_new, gates = self.cell(x_t, h, c)\n",
    "\n",
    "            h = mask_t * h_new + (1.0 - mask_t) * h\n",
    "            c = mask_t * c_new + (1.0 - mask_t) * c\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "class StatFromScratch(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.sequence = LSTMSequence(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "\n",
    "        h_last = self.sequence(x, lengths)   # (B, H)\n",
    "        out = self.fc(h_last).squeeze(-1)    # (B,) scalar prediction\n",
    "        return out, h_last\n",
    "\n",
    "\n",
    "\n",
    "class StatFromScratchBinary(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.sequence = LSTMSequence(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "\n",
    "        h_last = self.sequence(x, lengths)   # (B, H)\n",
    "        logits = self.fc(h_last).squeeze(-1) # (B,) raw logits\n",
    "        return logits, h_last\n",
    "\n",
    "\n",
    "def create_model_from_dataset(cfg: StatSeqConfig, device, stat_name: str, parlay_tgt: str):\n",
    "\n",
    "    df = load_stat_df(stat_name)\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    feature_names = [\n",
    "        c for c in numeric_cols\n",
    "        if c not in [\"season\"] and c != parlay_tgt   # exclude season + target\n",
    "    ]\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna(subset=feature_names + [parlay_tgt])\n",
    "\n",
    "    train_ds, test_ds = make_train_test_sequences(\n",
    "        df,\n",
    "        feature_cols=feature_names,\n",
    "        target_col=parlay_tgt,\n",
    "        window=cfg.n_past_games,\n",
    "    )\n",
    "\n",
    "    train_dl = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=stat_collate_fn,\n",
    "    )\n",
    "\n",
    "    test_dl = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=stat_collate_fn,\n",
    "    )\n",
    "\n",
    "    # 5) model\n",
    "    num_features = len(feature_names)\n",
    "    model = StatFromScratch(\n",
    "        input_size=num_features,\n",
    "        hidden_size=cfg.hidden_size\n",
    "    ).to(device)\n",
    "\n",
    "    return model, train_dl, test_dl, feature_names\n",
    "\n",
    "\n",
    "\n",
    "def create_binary_model_from_dataset(\n",
    "    cfg: StatSeqConfig,\n",
    "    device,\n",
    "    stat_name: str,\n",
    "    parlay_tgt: str,\n",
    "    threshold: float,\n",
    "):\n",
    "\n",
    "    df = load_stat_df(stat_name)\n",
    "\n",
    "    df[\"label\"] = (df[parlay_tgt] >= threshold).astype(\"float32\")\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    feature_names = [\n",
    "        c for c in numeric_cols\n",
    "        if c not in [\"season\", \"label\"] and c != parlay_tgt\n",
    "    ]\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna(subset=feature_names + [\"label\"])\n",
    "\n",
    "    train_ds, test_ds = make_train_test_sequences(\n",
    "        df,\n",
    "        feature_cols=feature_names,\n",
    "        target_col=\"label\",\n",
    "        window=cfg.n_past_games,\n",
    "    )\n",
    "\n",
    "    train_dl = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=stat_collate_fn,\n",
    "    )\n",
    "\n",
    "    test_dl = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=stat_collate_fn,\n",
    "    )\n",
    "\n",
    "    num_features = len(feature_names)\n",
    "    model = StatFromScratchBinary(\n",
    "        input_size=num_features,\n",
    "        hidden_size=cfg.hidden_size\n",
    "    ).to(device)\n",
    "\n",
    "    return model, train_dl, test_dl, feature_names\n",
    "\n",
    "\n",
    "def train_binary_model(\n",
    "    model,\n",
    "    train_dl,\n",
    "    test_dl,\n",
    "    cfg: StatSeqConfig,\n",
    "):\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "    for epoch in range(1, cfg.n_epochs + 1):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for X, lengths, y in train_dl:\n",
    "            X = X.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits, _ = model(X, lengths)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs >= 0.5).float()\n",
    "                train_correct += (preds == y).sum().item()\n",
    "                train_total += y.numel()\n",
    "\n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total * 100.0\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, lengths, y in test_dl:\n",
    "                X = X.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                logits, _ = model(X, lengths)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "                test_loss += loss.item() * X.size(0)\n",
    "\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs >= 0.5).float()\n",
    "                test_correct += (preds == y).sum().item()\n",
    "                test_total += y.numel()\n",
    "\n",
    "        test_loss /= test_total\n",
    "        test_acc = test_correct / test_total * 100.0\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d}/{cfg.n_epochs} | \"\n",
    "            f\"Train L={train_loss:.4f} A={train_acc:.1f}% | \"\n",
    "            f\"Test L={test_loss:.4f} A={test_acc:.1f}%\"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "def build_player_sequence(\n",
    "    df: pd.DataFrame,\n",
    "    player_name: str,\n",
    "    cfg: StatSeqConfig,\n",
    "    feature_names: List[str],\n",
    "):\n",
    "\n",
    "    g = df[df[\"display_name\"] == player_name].sort_values(\"date\")\n",
    "\n",
    "    g_last = g.tail(cfg.n_past_games)\n",
    "    X_np = g_last[feature_names].values.astype(\"float32\")  # (T, E)\n",
    "\n",
    "    T = X_np.shape[0]\n",
    "    X = torch.from_numpy(X_np).unsqueeze(0)      # (1, T, E)\n",
    "    lengths = torch.tensor([T], dtype=torch.long)   # (1,)\n",
    "\n",
    "    return X, lengths\n",
    "\n",
    "\n",
    "def predict_over_probability(\n",
    "    model: StatFromScratchBinary,\n",
    "    df: pd.DataFrame,\n",
    "    cfg: StatSeqConfig,\n",
    "    player_name: str,\n",
    "    feature_names: List[str],\n",
    "    device,\n",
    ") -> float:\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    X, lengths = build_player_sequence(df, player_name, cfg, feature_names)\n",
    "    X = X.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(X, lengths)      # (1,)\n",
    "        prob_over = torch.sigmoid(logits).item()\n",
    "\n",
    "    return prob_over\n",
    "\n",
    "def parlay_model_prob(\n",
    "    legs: List[Dict],\n",
    "    cfg: StatSeqConfig,\n",
    "    device,\n",
    ") -> float:\n",
    "\n",
    "    p_parlay = 1.0\n",
    "\n",
    "    for leg in legs:\n",
    "        model = leg[\"model\"]\n",
    "        df = leg[\"df\"]\n",
    "        player_name = leg[\"player_name\"]\n",
    "        feature_names = leg[\"feature_names\"]\n",
    "        side = leg[\"side\"].lower()\n",
    "\n",
    "        p_over = predict_over_probability(\n",
    "            model=model,\n",
    "            df=df,\n",
    "            cfg=cfg,\n",
    "            player_name=player_name,\n",
    "            feature_names=feature_names,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        if side == \"over\":\n",
    "            p_leg = p_over\n",
    "        elif side == \"under\":\n",
    "            p_leg = 1.0 - p_over\n",
    "        else:\n",
    "            raise ValueError(\"leg['side'] must be 'over' or 'under'\")\n",
    "\n",
    "        p_parlay *= p_leg\n",
    "\n",
    "    return p_parlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Leg Parlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train L=0.5825 A=80.1% | Test L=0.4512 A=87.9%\n",
      "Epoch 02/20 | Train L=0.4876 A=81.7% | Test L=0.3952 A=87.9%\n",
      "Epoch 03/20 | Train L=0.4770 A=81.7% | Test L=0.3846 A=87.9%\n",
      "Epoch 04/20 | Train L=0.4768 A=81.7% | Test L=0.3833 A=87.9%\n",
      "Epoch 05/20 | Train L=0.4768 A=81.7% | Test L=0.3819 A=87.9%\n",
      "Epoch 06/20 | Train L=0.4767 A=81.7% | Test L=0.3824 A=87.9%\n",
      "Epoch 07/20 | Train L=0.4769 A=81.7% | Test L=0.3824 A=87.9%\n",
      "Epoch 08/20 | Train L=0.4769 A=81.7% | Test L=0.3850 A=87.9%\n",
      "Epoch 09/20 | Train L=0.4767 A=81.7% | Test L=0.3823 A=87.9%\n",
      "Epoch 10/20 | Train L=0.4767 A=81.7% | Test L=0.3832 A=87.9%\n",
      "Epoch 11/20 | Train L=0.4769 A=81.7% | Test L=0.3849 A=87.9%\n",
      "Epoch 12/20 | Train L=0.4768 A=81.7% | Test L=0.3828 A=87.9%\n",
      "Epoch 13/20 | Train L=0.4768 A=81.7% | Test L=0.3827 A=87.9%\n",
      "Epoch 14/20 | Train L=0.4770 A=81.7% | Test L=0.3830 A=87.9%\n",
      "Epoch 15/20 | Train L=0.4768 A=81.7% | Test L=0.3857 A=87.9%\n",
      "Epoch 16/20 | Train L=0.4769 A=81.7% | Test L=0.3819 A=87.9%\n",
      "Epoch 17/20 | Train L=0.4769 A=81.7% | Test L=0.3824 A=87.9%\n",
      "Epoch 18/20 | Train L=0.4768 A=81.7% | Test L=0.3841 A=87.9%\n",
      "Epoch 19/20 | Train L=0.4767 A=81.7% | Test L=0.3835 A=87.9%\n",
      "Epoch 20/20 | Train L=0.4768 A=81.7% | Test L=0.3832 A=87.9%\n",
      "0.18409408628940582\n"
     ]
    }
   ],
   "source": [
    "cfg = StatSeqConfig()\n",
    "\n",
    "model, train_dl, test_dl, feature_names = create_binary_model_from_dataset(\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    stat_name=\"passing\",\n",
    "    parlay_tgt=\"YDS\",\n",
    "    threshold=305.5,\n",
    ")\n",
    "\n",
    "model = train_binary_model(model, train_dl, test_dl, cfg)\n",
    "\n",
    "df_pass = load_stat_df(\"passing\")\n",
    "\n",
    "prob = predict_over_probability(\n",
    "    model=model,\n",
    "    df=df_pass,\n",
    "    cfg=cfg,\n",
    "    player_name=\"Patrick Mahomes\",\n",
    "    feature_names=feature_names,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Leg Parlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train L=0.5483 A=81.7% | Test L=0.4406 A=87.9%\n",
      "Epoch 02/20 | Train L=0.4842 A=81.7% | Test L=0.3970 A=87.9%\n",
      "Epoch 03/20 | Train L=0.4786 A=81.7% | Test L=0.3893 A=87.9%\n",
      "Epoch 04/20 | Train L=0.4782 A=81.7% | Test L=0.3869 A=87.9%\n",
      "Epoch 05/20 | Train L=0.4780 A=81.7% | Test L=0.3889 A=87.9%\n",
      "Epoch 06/20 | Train L=0.4777 A=81.7% | Test L=0.3853 A=87.9%\n",
      "Epoch 07/20 | Train L=0.4775 A=81.7% | Test L=0.3853 A=87.9%\n",
      "Epoch 08/20 | Train L=0.4773 A=81.7% | Test L=0.3849 A=87.9%\n",
      "Epoch 09/20 | Train L=0.4774 A=81.7% | Test L=0.3858 A=87.9%\n",
      "Epoch 10/20 | Train L=0.4770 A=81.7% | Test L=0.3844 A=87.9%\n",
      "Epoch 11/20 | Train L=0.4768 A=81.7% | Test L=0.3830 A=87.9%\n",
      "Epoch 12/20 | Train L=0.4768 A=81.7% | Test L=0.3830 A=87.9%\n",
      "Epoch 13/20 | Train L=0.4767 A=81.7% | Test L=0.3817 A=87.9%\n",
      "Epoch 14/20 | Train L=0.4766 A=81.7% | Test L=0.3807 A=87.9%\n",
      "Epoch 15/20 | Train L=0.4766 A=81.7% | Test L=0.3813 A=87.9%\n",
      "Epoch 16/20 | Train L=0.4764 A=81.7% | Test L=0.3806 A=87.9%\n",
      "Epoch 17/20 | Train L=0.4763 A=81.7% | Test L=0.3834 A=87.9%\n",
      "Epoch 18/20 | Train L=0.4761 A=81.7% | Test L=0.3803 A=87.9%\n",
      "Epoch 19/20 | Train L=0.4763 A=81.7% | Test L=0.3793 A=87.9%\n",
      "Epoch 20/20 | Train L=0.4760 A=81.7% | Test L=0.3790 A=87.9%\n",
      "Epoch 01/20 | Train L=0.5199 A=79.9% | Test L=0.4932 A=80.5%\n",
      "Epoch 02/20 | Train L=0.5018 A=79.9% | Test L=0.4933 A=80.5%\n",
      "Epoch 03/20 | Train L=0.5018 A=79.9% | Test L=0.4933 A=80.5%\n",
      "Epoch 04/20 | Train L=0.5018 A=79.9% | Test L=0.4939 A=80.5%\n",
      "Epoch 05/20 | Train L=0.5018 A=79.9% | Test L=0.4933 A=80.5%\n",
      "Epoch 06/20 | Train L=0.5018 A=79.9% | Test L=0.4940 A=80.5%\n",
      "Epoch 07/20 | Train L=0.5017 A=79.9% | Test L=0.4933 A=80.5%\n",
      "Epoch 08/20 | Train L=0.5016 A=79.9% | Test L=0.4933 A=80.5%\n",
      "Epoch 09/20 | Train L=0.5017 A=79.9% | Test L=0.4938 A=80.5%\n",
      "Epoch 10/20 | Train L=0.5018 A=79.9% | Test L=0.4933 A=80.5%\n",
      "Epoch 11/20 | Train L=0.5019 A=79.9% | Test L=0.4935 A=80.5%\n",
      "Epoch 12/20 | Train L=0.5019 A=79.9% | Test L=0.4936 A=80.5%\n",
      "Epoch 13/20 | Train L=0.5018 A=79.9% | Test L=0.4933 A=80.5%\n",
      "Epoch 14/20 | Train L=0.5017 A=79.9% | Test L=0.4936 A=80.5%\n",
      "Epoch 15/20 | Train L=0.5019 A=79.9% | Test L=0.4936 A=80.5%\n",
      "Epoch 16/20 | Train L=0.5020 A=79.9% | Test L=0.4935 A=80.5%\n",
      "Epoch 17/20 | Train L=0.5018 A=79.9% | Test L=0.4942 A=80.5%\n",
      "Epoch 18/20 | Train L=0.5015 A=79.9% | Test L=0.4932 A=80.5%\n",
      "Epoch 19/20 | Train L=0.5020 A=79.9% | Test L=0.4939 A=80.5%\n",
      "Epoch 20/20 | Train L=0.5018 A=79.9% | Test L=0.4933 A=80.5%\n",
      "Model P(Mahomes o305.5 & Pacheco o55.5) = 0.035\n"
     ]
    }
   ],
   "source": [
    "cfg = StatSeqConfig()\n",
    "mahomes_name = \"Patrick Mahomes\"\n",
    "pacheco_name = \"Isiah Pacheco\"\n",
    "\n",
    "pass_threshold = 305.5  \n",
    "\n",
    "model_pass, train_dl_pass, test_dl_pass, feat_pass = create_binary_model_from_dataset(\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    stat_name=\"passing\",\n",
    "    parlay_tgt=\"YDS\",      \n",
    "    threshold=pass_threshold,\n",
    ")\n",
    "model_pass = train_binary_model(\n",
    "    model=model_pass,\n",
    "    train_dl=train_dl_pass,\n",
    "    test_dl=test_dl_pass,\n",
    "    cfg=cfg,\n",
    ")\n",
    "df_pass = load_stat_df(\"passing\")\n",
    "\n",
    "rush_threshold = 55.5   \n",
    "\n",
    "model_rush, train_dl_rush, test_dl_rush, feat_rush = create_binary_model_from_dataset(\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    stat_name=\"rushing\",\n",
    "    parlay_tgt=\"YDS\",     \n",
    "    threshold=rush_threshold,\n",
    ")\n",
    "\n",
    "model_rush = train_binary_model(\n",
    "    model=model_rush,\n",
    "    train_dl=train_dl_rush,\n",
    "    test_dl=test_dl_rush,\n",
    "    cfg=cfg,\n",
    ")\n",
    "\n",
    "df_rush = load_stat_df(\"rushing\")\n",
    "\n",
    "\n",
    "legs = [\n",
    "    {\n",
    "        \"model\": model_pass,\n",
    "        \"df\": df_pass,\n",
    "        \"player_name\": mahomes_name,\n",
    "        \"feature_names\": feat_pass,\n",
    "        \"side\": \"over\",   # Mahomes over 305.5\n",
    "    },\n",
    "    {\n",
    "        \"model\": model_rush,\n",
    "        \"df\": df_rush,\n",
    "        \"player_name\": pacheco_name,\n",
    "        \"feature_names\": feat_rush,\n",
    "        \"side\": \"over\",   # Pacheco over 55.5\n",
    "    },\n",
    "]\n",
    "\n",
    "p_parlay = parlay_model_prob(legs=legs, cfg=cfg, device=device)\n",
    "print(f\"Model P(Mahomes o{pass_threshold} & Pacheco o{rush_threshold}) = {p_parlay:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Kelly ROI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def kelly_fraction_even(p: float) -> float:\n",
    "\n",
    "    return max(0.0, 2.0 * p - 1.0)\n",
    "\n",
    "\n",
    "def sample_parlays(\n",
    "    hat_p: np.ndarray,     # predicted p_i for each leg (N,)\n",
    "    y_true: np.ndarray,    # true outcomes for each leg (N,)\n",
    "    L: int,                # parlay size\n",
    "    M: int,                # number of parlays to sample\n",
    "    rng: np.random.Generator = None,\n",
    "):\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    N = len(hat_p)\n",
    "    parlay_probs = np.zeros(M, dtype=np.float32)\n",
    "    parlay_outcomes = np.zeros(M, dtype=np.float32)\n",
    "\n",
    "    for m in range(M):\n",
    "        idx = rng.choice(N, size=L, replace=False)\n",
    "\n",
    "        # predicted parlay probability = product of single-leg probs\n",
    "        p_parlay = float(np.prod(hat_p[idx]))\n",
    "\n",
    "        # actual parlay outcome = 1 if ALL legs hit\n",
    "        y_parlay = float(np.all(y_true[idx] == 1))\n",
    "\n",
    "        parlay_probs[m] = p_parlay\n",
    "        parlay_outcomes[m] = y_parlay\n",
    "\n",
    "    return parlay_probs, parlay_outcomes\n",
    "\n",
    "\n",
    "def parlay_calibration_error(\n",
    "    parlay_probs: np.ndarray,\n",
    "    parlay_outcomes: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Parlay Calibration Error (PCE):\n",
    "      ε_PCE = (1/M) Σ | p_parlay - y_parlay |\n",
    "    \"\"\"\n",
    "    return float(np.mean(np.abs(parlay_probs - parlay_outcomes)))\n",
    "\n",
    "\n",
    "def compute_kelly_and_flat_roi(\n",
    "    parlay_probs: np.ndarray,     # predicted parlay probabilities\n",
    "    parlay_outcomes: np.ndarray,  # realized outcomes\n",
    "):\n",
    "\n",
    "    profit_flat = parlay_outcomes * 1.0 - 1.0   # (M,)\n",
    "\n",
    "    kelly_fractions = np.array(\n",
    "        [kelly_fraction_even(p) for p in parlay_probs],\n",
    "        dtype=np.float32\n",
    "    )  # (M,)\n",
    "\n",
    "    profit_kelly = kelly_fractions * (parlay_outcomes * 1.0 - 1.0)\n",
    "\n",
    "    return {\n",
    "        \"flat_roi\": float(np.mean(profit_flat)),\n",
    "        \"kelly_roi\": float(np.mean(profit_kelly)),\n",
    "        \"avg_kelly_fraction\": float(np.mean(kelly_fractions)),\n",
    "    }\n",
    "\n",
    "def collect_leg_predictions(model, test_dl, device):\n",
    "\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, lengths, y in test_dl:\n",
    "            X = X.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits, _ = model(X, lengths)     # (B,)\n",
    "            probs = torch.sigmoid(logits)     # (B,) → probabilities\n",
    "\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(y.cpu().numpy())\n",
    "\n",
    "    hat_p = np.concatenate(all_probs, axis=0)\n",
    "    y_true = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return hat_p, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCE: 0.048221878707408905\n",
      "Flat ROI: -0.9819999933242798\n",
      "Kelly ROI: 0.0\n",
      "Avg Kelly Fraction: 0.0\n"
     ]
    }
   ],
   "source": [
    "hat_p, y_true = collect_leg_predictions(model_pass, test_dl_pass, device)\n",
    "L = 2\n",
    "M = 1000\n",
    "\n",
    "parlay_probs, parlay_outcomes = sample_parlays(hat_p, y_true, L=L, M=M)\n",
    "\n",
    "pce = parlay_calibration_error(parlay_probs, parlay_outcomes)\n",
    "rois = compute_kelly_and_flat_roi(parlay_probs, parlay_outcomes)\n",
    "\n",
    "print(\"PCE:\", pce)\n",
    "print(\"Flat ROI:\", rois[\"flat_roi\"])\n",
    "print(\"Kelly ROI:\", rois[\"kelly_roi\"])\n",
    "print(\"Avg Kelly Fraction:\", rois[\"avg_kelly_fraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_calibration_error(\n",
    "    hat_p: np.ndarray,\n",
    "    y_true: np.ndarray,\n",
    "    n_bins: int = 10,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Standard Expected Calibration Error (ECE) for binary predictions.\n",
    "\n",
    "    hat_p:  predicted probabilities, shape (N,)\n",
    "    y_true: true labels in {0,1}, shape (N,)\n",
    "    n_bins: number of confidence bins\n",
    "    \"\"\"\n",
    "    assert hat_p.shape == y_true.shape\n",
    "    N = len(hat_p)\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        start, end = bins[i], bins[i + 1]\n",
    "        mask = (hat_p >= start) & (hat_p < end)\n",
    "        n_bin = mask.sum()\n",
    "        if n_bin == 0:\n",
    "            continue\n",
    "\n",
    "        conf_bin = float(hat_p[mask].mean())\n",
    "        acc_bin = float(y_true[mask].mean())\n",
    "        ece += (n_bin / N) * abs(conf_bin - acc_bin)\n",
    "\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def run_parlay_experiment(\n",
    "    model,\n",
    "    test_dl,\n",
    "    device,\n",
    "    L: int = 2,\n",
    "    M: int = 1000,\n",
    "    n_bins: int = 10,\n",
    "    rng: np.random.Generator = None,\n",
    "):\n",
    "\n",
    "    hat_p, y_true = collect_leg_predictions(model, test_dl, device)\n",
    "\n",
    "    ece = expected_calibration_error(hat_p, y_true, n_bins=n_bins)\n",
    "\n",
    "    parlay_probs, parlay_outcomes = sample_parlays(\n",
    "        hat_p=hat_p,\n",
    "        y_true=y_true,\n",
    "        L=L,\n",
    "        M=M,\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "    pce = parlay_calibration_error(parlay_probs, parlay_outcomes)\n",
    "\n",
    "    rois = compute_kelly_and_flat_roi(parlay_probs, parlay_outcomes)\n",
    "\n",
    "    print(\"===== Parlay Experiment Summary =====\")\n",
    "    print(f\"Single-leg ECE:          {ece:.4f}\")\n",
    "    print(f\"L-leg PCE (L={L}):       {pce:.4f}\")\n",
    "    print(f\"Flat ROI per parlay:     {rois['flat_roi']:.4f}\")\n",
    "    print(f\"Kelly ROI per parlay:    {rois['kelly_roi']:.4f}\")\n",
    "    print(f\"Avg Kelly fraction:      {rois['avg_kelly_fraction']:.4f}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    results = {\n",
    "        \"ece\": ece,\n",
    "        \"pce\": pce,\n",
    "        \"flat_roi\": rois[\"flat_roi\"],\n",
    "        \"kelly_roi\": rois[\"kelly_roi\"],\n",
    "        \"avg_kelly_fraction\": rois[\"avg_kelly_fraction\"],\n",
    "        \"hat_p\": hat_p,\n",
    "        \"y_true\": y_true,\n",
    "        \"parlay_probs\": parlay_probs,\n",
    "        \"parlay_outcomes\": parlay_outcomes,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Parlay Experiment Summary =====\n",
      "Single-leg ECE:          0.0567\n",
      "L-leg PCE (L=2):       0.0464\n",
      "Flat ROI per parlay:     -0.9840\n",
      "Kelly ROI per parlay:    0.0000\n",
      "Avg Kelly fraction:      0.0000\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = run_parlay_experiment(\n",
    "    model=model_pass,\n",
    "    test_dl=test_dl_pass,\n",
    "    device=device,\n",
    "    L=2,       # 2-leg parlays\n",
    "    M=1000,    # 1000 sampled parlays\n",
    "    n_bins=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
